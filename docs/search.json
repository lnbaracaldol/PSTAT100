[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSTAT100",
    "section": "",
    "text": "Announcements\n\n\n\n\nDiscussion sections start on Thursday 06/29\n\n\n\nThis is the course website for UCSB's Data Science Concepts and Analysis class (PSTAT100). Content is directed towards currently enrolled students. Please ask permission before using course materials in any other capacity. Please do not post any materials obtained or derived from this website on third-party websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Instructor: Laura Baracaldo [email]\nTeaching assistants: Doris Padilla [email] and Erika McPhillips [email].\n\nThe instructor is primarily responsible for delivering lectures and course administration. The teaching assistants are primarily responsible for providing lab instruction and managing assessments and course pages. All staff hold regular office hours on a weekly basis open to all enrolled students.\n\n\n\n\n\n\n\n\n\nTR 11:00am - 11:50am, HSSB 1232 (Doris)\nTR 12:30pm - 1:20pm, HSSB 1215 (Erika)\nTR 10:00am - 10:50am, HSSB 1232 (Erika)\n\n\n\n\n\nLaura Baracaldo: Old Gym 1201. W 12:00-1:15pm, or via zoom"
  },
  {
    "objectID": "about.html#learning-outcomes",
    "href": "about.html#learning-outcomes",
    "title": "Course syllabus",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nSuccessful students will establish foundational data science skills: critical assessment of data quality and sampling design; inspection and tidying of raw data; exploratory, descriptive, visual, and inferential analysis; and interpretation and communication of results for a general audience.\nThese skills will be discussed in depth during course lectures; students will practice them through lab activities, homework assignments, and project work."
  },
  {
    "objectID": "about.html#ucsb-catalog-listing",
    "href": "about.html#ucsb-catalog-listing",
    "title": "Course syllabus",
    "section": "UCSB catalog listing",
    "text": "UCSB catalog listing\n\nDescription\nOverview of data science key concepts and the use of tools for data retrieval, analysis, visualization, and reproducible research. Topics include an introduction to inference and prediction, principles of measurement, missing data, and notions of causality, statistical “traps”, and concepts in data ethics and privacy. Case studies will illustrate the importance of domain knowledge.\n\n\nPrerequisites\n\nProbability and Statistics I (PSTAT 120A)\nLinear Algebra (MATH 4A)\nPrior experience with Python or another programming language (CMPSC 9 or CMPSC 16). Credit units: 4.\n\n\n\nReadings\nReadings for the course will draw on multiple sources, including:\n\nPython Data Science Handbook;\nBerkeley’s Data 100 course notes;\ncollected journal articles distributed as assigned.\n\n\n\nSoftware\nComputing will be hosted at pstat100.lsit.ucsb.edu.\nStudents are encouraged to install software needed to open, edit, and run notebooks on their own machine, in particular:\n\na Python install;\nJupyter;\npackages utilized in course materials (primarily numpy, pandas, altair, and scikit-learn).\n\nManaging package installations will require some (straightforward) use of the package installer pip or pip3 in the command line to retrieve/install packages from the Python Package Index repository. Documentation for specific packages (or a Google search) will indicate the appropriate pip command."
  },
  {
    "objectID": "about.html#spring-2022-information",
    "href": "about.html#spring-2022-information",
    "title": "Course syllabus",
    "section": "",
    "text": "Instructor: Laura Baracaldo\nTeaching assistants: Han Yu, Mengye Liu and Abhijit Brahme.\n\nThe instructor is primarily responsible for delivering lectures and course administration. The teaching assistants are primarily responsible for providing lab instruction and managing assessments and course pages. All staff hold regular office hours on a weekly basis open to all enrolled students.\n\n\n\nIn Winter 2023, lectures are offered in Person, MW 8:00-9:15am IV THEA2; sections are held as scheduled on GOLD.\n\n\n\nLaura Baracaldo: Old Gym 1201. W 2:00-3:00pm."
  },
  {
    "objectID": "about.html#materials",
    "href": "about.html#materials",
    "title": "Course syllabus",
    "section": "Materials",
    "text": "Materials\nThe canvas page will link to all course content and resources. Content will be released and collected according to the following weekly pattern:\n\nMondays at 8am : week’s content released.\nFridays at 11:59pm : assignment(s) due."
  },
  {
    "objectID": "about.html#assessments",
    "href": "about.html#assessments",
    "title": "Course syllabus",
    "section": "Assessments",
    "text": "Assessments\nAttainment of course learning outcomes will be measured by assessment of submitted work. Submitted work falls into four categories:\n\nLabs will be assigned weekly in most weeks. These are structured coding assignments with small exercises throughout that introduce the programming skills needed to complete homework assignments.\nHomeworks will be assigned weekly. These are fairly involved assignments which apply concepts and techniques from the lectures and programming skills from the labs to real data sets in order to reproduce an analysis and answer substantive questions. Collaboration is encouraged, and group submissions will be allowed for groups of up to 3 students.\nMini projects These assignments prompt students to use skills from the course in an unstructured setting to answer high-level questions pertaining to one or more datasets. Mini projects should be completed individually, but discussion is permitted and encouraged.\nA course project will be assigned requiring students to carry out an open-ended data analysis. This will be completed in teams.\n\nOverall scores in the course will be calculated for each student as the weighted average of scores on all submitted work; the relative weighting and letter grade assignments will depend entirely on the score distribution of the class as a whole and as such reflect each student’s performance relative to their peers."
  },
  {
    "objectID": "about.html#schedule",
    "href": "about.html#schedule",
    "title": "Course syllabus",
    "section": "Schedule",
    "text": "Schedule\nThe tentative topic and assignment schedule is given below. All assignments are due by Friday 11:59pm in the week indicated; late submissions are allowed, with a possible penalty, through the following Monday at 11:59pm.\nThe schedule is subject to change based on the progress of the class.\nWeek | Topic | Lab | Homework | Project\n—|—|—|—|—\n0 | Data science life cycle | L0 | |\n1 | Tidy data | L1 | |\n2 | Sampling and bias | L2 | H1\n3 | Statistical graphics | L3 | | MP1\n4 | Kernel density estimation | L4 | H2\n5 | Principal components | L5 | | MP2\n6 | Simple regression | | H3\n7 | Multiple regression | L6 | | CP1\n8 | Case study and developing ‘presentables’ | | H4\n9 | Closing | | |\n10 | Finals | | | CP2\n*L: lab\n*H: homework\n*MP: mini project\n*CP: course project"
  },
  {
    "objectID": "about.html#policies",
    "href": "about.html#policies",
    "title": "Course syllabus",
    "section": "Policies",
    "text": "Policies\n\nCommunication\nThere are two primary means of communication outside of scheduled class meetings: office hours and Nectir\n\nEmail\nCourse staff have limited availability via email. Course staff will make every effort to respond to individual communication within 48 weekday-hours on the following (or similar) matters:\n\naccommodations/extensions due to personal circumstances;\nlogistical issues such as access to materials or missing scores;\ngeneral advising.\n\nEmail should not be used to ask content questions or submit assignments (unless specifically requested). Emails related to the following (or similar) matters may not receive replies and should be redirected:\n\nTroubleshooting codes: Nectir\nChecking answers: OH or Nectir\nClarifying assignment content: OH or Nectir\nAssignment submission: Gradescope\nRe-evaluation request: Gradescope\n\n\n\n\nExpected time commitment\nThe course is 4 credit units; each credit unit corresponds to an approximate time commitment of 3 hours. So, students should expect to allocate 12 hours per week to the course on average. Course staff are available to help any students spending considerably more time on the class balance the workload.\n\n\nScores and grades\nScores on submitted work can be monitored on Gradescope to ensure fair assignment of course grades. On any individual assignment, re-evaluation can be requested within one week of receiving a score. Requests for re-evaluation made beyond one week after publication of scores may or may not be considered on a discretionary basis.\nDetermination of letter grade assignments is made entirely at the discretion of the instructor based on the assessments outlined above and consistent with university policy. Students are not permitted to negotiate their grades, and are discouraged from requesting audits, recalculations, or verification of self-calculations after the course has concluded. The instructor is under no obligation to share the details of grade calculations with students or to respond to such requests.\nIf at the end of the course a student believes their grade was unfairly assigned, either due to discrimination or without basis in coursework, they are entitled to contest it according to the procedure outlined here.\n\n\nConduct\nStudents are expected to uphold the student code of conduct and to maintain integrity. All individually-submitted work must be an honest reflection of individual effort. Evidence of dishonest conduct will be discussed with the student(s) involved and reported to the Office of Student Conduct (OSC). Depending on the nature of the evidence and the violation, penalty in the course may range from a warning to loss of credit to automatic failure. For a definition and examples of dishonesty, a discussion of what constitutes an appropriate response from faculty, and an explanation of the reporting and investigation process, see the OSC page on academic integrity.\n\n\nDeadlines and late work\nThere is a one-hour grace period on all submission deadlines. Late work will not be accepted beyond this.\n\n\nAccommodations\nReasonable accommodations will be made for any student with a qualifying disability. Such requests should be made through the Disabled Students Program (DSP). More information, instructions on how to access accommodations, and information on related resources can be found on the DSP website.\n\n\nFeedback\nToward the end of the term students will be given an opportunity to provide feedback about the course via ESCI. In addition, content-specific feedback will be collected for the Central Coast Data Science Partnership (CCDSP) at the end of the quarter. This information will be used to assess learning outcomes, understand student demographics, and plan further course development; input on the CCDSP survey is especially valuable, and a small amount of course credit will be offered for completion of this survey."
  },
  {
    "objectID": "about.html#summer-2023-information",
    "href": "about.html#summer-2023-information",
    "title": "Course syllabus",
    "section": "",
    "text": "Instructor: Laura Baracaldo [email]\nTeaching assistants: Doris Padilla [email] and Erika McPhillips [email].\n\nThe instructor is primarily responsible for delivering lectures and course administration. The teaching assistants are primarily responsible for providing lab instruction and managing assessments and course pages. All staff hold regular office hours on a weekly basis open to all enrolled students.\n\n\n\n\n\n\n\n\n\nTR 11:00am - 11:50am, HSSB 1232 (Doris)\nTR 12:30pm - 1:20pm, HSSB 1215 (Erika)\nTR 10:00am - 10:50am, HSSB 1232 (Erika)\n\n\n\n\n\nLaura Baracaldo: Old Gym 1201. W 12:00-1:15pm, or via zoom"
  },
  {
    "objectID": "about.html#content-and-materials",
    "href": "about.html#content-and-materials",
    "title": "Course syllabus",
    "section": "Content and Materials",
    "text": "Content and Materials\nData Science Concepts and Analysis (PSTAT100) is a hands-on introduction to data science intended for intermediate-level students from any discipline with some exposure to probability and basic computing skills, but few or no upper-division courses in statistics or computer science. The course introduces central concepts in statistics – such as sampling variation, uncertainty, and inference – in an applied setting together with techniques for data exploration and analysis. Course activities model standard data science workflow practices by example, and successful students acquire programming skills, project management skills, and subject exposure that will serve them well in upper-division courses as well as in independent research or projects.\n\nLearning outcomes\nSuccessful students will establish foundational data science skills: critical assessment of data quality and sampling design; inspection and tidying of raw data; exploratory, descriptive, visual, and inferential analysis; and interpretation and communication of results for a general audience.\nThese skills will be discussed in depth during course lectures; students will practice them through lab activities, homework assignments, and project work.\n\n\nUCSB catalog listing\n\n\nDescription\nOverview of data science key concepts and the use of tools for data retrieval, analysis, visualization, and reproducible research. Topics include an introduction to inference and prediction, principles of measurement, missing data, and notions of causality, statistical “traps”, and concepts in data ethics and privacy. Case studies will illustrate the importance of domain knowledge.\n\n\nPrerequisites\n\nProbability and Statistics I (PSTAT 120A)\nLinear Algebra (MATH 4A)\nPrior experience with Python or another programming language (CMPSC 9 or CMPSC 16). Credit units: 4.\n\n\n\nReadings\nReadings for the course will draw on multiple sources, including:\n\nPython Data Science Handbook;\nBerkeley’s Data 100 course notes;\ncollected journal articles distributed as assigned.\n\n\n\nSoftware\nComputing will be hosted at pstat100.lsit.ucsb.edu.\nStudents are encouraged to install software needed to open, edit, and run notebooks on their own machine, in particular:\n\na Python install;\n(recommended) Miniconda;\nJupyter;\npackages utilized in course materials (primarily numpy, pandas, altair, and scikit-learn).\n\nManaging package installations will require some (straightforward) use of the package installer pip or pip3 in the command line to retrieve/install packages from the Python Package Index repository. Documentation for specific packages (or a Google search) will indicate the appropriate pip command."
  },
  {
    "objectID": "about.html#topics-and-corresponding-assignmets",
    "href": "about.html#topics-and-corresponding-assignmets",
    "title": "Course syllabus",
    "section": "Topics and corresponding assignmets:",
    "text": "Topics and corresponding assignmets:\nThe schedule is subject to change based on the progress of the class.\n\nData science life cycle: Lab0\nTidy data: Lab1\nSampling and bias: Lab2, Hw1\nStatistical graphics: Lab 3, Mini Project\nKernel density estimation: Lab 4, Homework 2.\nPrincipal components: Lab 5\nSimple regression: Hw3\nMultiple regression: Lab 6\nCase study and developing ‘presentables’: Final Project"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Materials",
    "section": "",
    "text": "Confirm access to all course pages\nRead syllabus"
  },
  {
    "objectID": "materials.html#getting-started-checklist",
    "href": "materials.html#getting-started-checklist",
    "title": "Materials",
    "section": "",
    "text": "Confirm access to all course pages\nRead syllabus"
  },
  {
    "objectID": "materials.html#week-1",
    "href": "materials.html#week-1",
    "title": "Materials",
    "section": "Week 1",
    "text": "Week 1\nReadings:\n\nThe Data Science Lifecycle\nUnderstanding data types in python\nThe basics of numpy arrays\nAggregations: min, max, and everything in between\n\nTuesday: [slides]\nLab section: Lab 0 zip\nWednesday-Thursday[slides]\nLab section: Lab 1 [zip][html]"
  },
  {
    "objectID": "materials.html#week-1-1",
    "href": "materials.html#week-1-1",
    "title": "Materials",
    "section": "Week 1",
    "text": "Week 1\nMonday-Thursday[slides pdf]"
  },
  {
    "objectID": "materials.html#week-2",
    "href": "materials.html#week-2",
    "title": "Materials",
    "section": "Week 2",
    "text": "Week 2\nReadings:\n\nLDS2.2 Population, frame, sample\nVan Buuren, Flexible Imputation of Missing Data, section 2.2 Concepts in incomplete data\nPDSH3.4 Handling missing data\n\nMonday-Thursday[slides pdf]\nHw1 [zip], [html], due on Tuesday 07/11.\nLab2[html], [zip], due on Thursday 07/13."
  },
  {
    "objectID": "materials.html#week-3",
    "href": "materials.html#week-3",
    "title": "Materials",
    "section": "Week 3:",
    "text": "Week 3:\nReadings:\n\nWilke, Fundamentals of Data Visualization Ch. 2-5\nChoosing scale to reveal structure\n(Recommended) Cook, D., Lee, E. K., & Majumder, M. (2016). Data visualization and statistical graphics in big data analysis. Annual Review of Statistics and Its Application, 3, 133-159. [link to paper]\n(Recommended) Gelman, A., & Unwin, A. (2013). Infovis and statistical graphics: different goals, different looks. Journal of Computational and Graphical Statistics, 22(1), 2-28. [link to paper]\n\nMonday-Tuesday[slides pdf]\nWednesday-Thursday[slides]\nLab3[html], [zip], due on Monday 07/17.\nMini-Project1 [zip], [html], due on Thursday 07/20."
  },
  {
    "objectID": "materials.html#week-4",
    "href": "materials.html#week-4",
    "title": "Materials",
    "section": "Week 4:",
    "text": "Week 4:\nReadings:\n\nExploratory data analysis\nPrincipal Component Analysis\n\nMonday-Thursday[slides pdf]\nLab4 [html], [zip], due on Monday 07/24.\nHw2 [zip], [html], due on Thursday 07/27."
  },
  {
    "objectID": "materials.html#week-5",
    "href": "materials.html#week-5",
    "title": "Materials",
    "section": "Week 5:",
    "text": "Week 5:\nReadings:\n\nSimple linear models\n\nMonday-Thursday[slides 1][slides 2]\nLab5 [html], [zip], due on Monday 07/31.\nHw3 [zip], [html], due on Thursday 08/03.\nClass Project[zip], [html],Groups spreadsheet due on Saturday 08/05."
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#this-week-the-simple-linear-model",
    "href": "docs/Lectures/week7-lse.html#this-week-the-simple-linear-model",
    "title": "Modeling concepts; least squares",
    "section": "This week: the simple linear model",
    "text": "This week: the simple linear model\n\nStatistical models\n\nWhat makes a model ‘statistical’?\nGoals of modeling: prediction, description, and inference\nWhen to avoid models\n\nThe simple linear regression model\n\nLine of best fit by least squares\nA model for the error distribution\nThe simple linear model\nInterpretation of estimates\nUncertainty quantification"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#what-makes-a-model",
    "href": "docs/Lectures/week7-lse.html#what-makes-a-model",
    "title": "Modeling concepts; least squares",
    "section": "What makes a model?",
    "text": "What makes a model?\n\nA model is an idealized representation of a system. You likely use models every day. For instance, a weather forecast is [based on] a model.\n\nIn the context of quantitative methods a model is typically a mathematical representation of some system.\n\nQuick discussion:\n\nwhat are some examples of models you’ve encountered?\nin what sense are they models?\nare they statistical models?"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#what-makes-a-model-statistical",
    "href": "docs/Lectures/week7-lse.html#what-makes-a-model-statistical",
    "title": "Modeling concepts; least squares",
    "section": "What makes a model ‘statistical’?",
    "text": "What makes a model ‘statistical’?\nOne straightforward view is that a statistical model is simply a probabilistic representation of a data-generating process. In other words, a probability distribution.\n\nFor a probability distribution to provide a sensible description of a data generating process:\n\none needs to be able to at least imagine collecting multiple datasets with the same basic structure\nobservations must be subject to randomness in some sense\n\n\n\nThat’s why sampling is so important to statisticians. Probabilistic sampling ensures:\n\nthe process by which data are collected is fixed and repeatable\nthe method of selection and measurement of observational units induces randomness in the data"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#univariate-models",
    "href": "docs/Lectures/week7-lse.html#univariate-models",
    "title": "Modeling concepts; least squares",
    "section": "Univariate models",
    "text": "Univariate models\nSuppose you have a dataset comprising the number of meteorites that hit earth on each of 225 days. A very simple model is that the counts are independent Poisson random variables.\n\n\n\n\n\nthe model is parametric, with a single parameter \\(\\lambda\\), given by the Poisson distribution: \\(f(x; \\lambda) = \\lambda^{x} e^{-\\lambda}/x!\\)\nsince \\(\\mathbb{E}X = \\lambda\\) and observations are assumed independent and identically distributed, the model is ‘fitted’ by estimating \\(\\lambda = \\bar{x}\\)"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#simple-example",
    "href": "docs/Lectures/week7-lse.html#simple-example",
    "title": "Modeling concepts; least squares",
    "section": "Simple example",
    "text": "Simple example\nHowever, the negative binomial distribution provides a better ‘fit’:\n\n\nperhaps not surprising, considering it has two parameters"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#why-model-the-data-generating-process",
    "href": "docs/Lectures/week7-lse.html#why-model-the-data-generating-process",
    "title": "Modeling concepts; least squares",
    "section": "Why model the data-generating process?",
    "text": "Why model the data-generating process?\nA good description of a data-generating process usually captures two aspects of a system:\n\nthe deterministic aspects, allowing one to identify structure in the data; and\nthe random aspects or ‘noise’, allowing one to quantify uncertainty.\n\n\nIn our toy example, the better model captured both the most common value (a kind of structure) and the variation (noise)."
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#modeling-goals",
    "href": "docs/Lectures/week7-lse.html#modeling-goals",
    "title": "Modeling concepts; least squares",
    "section": "Modeling goals",
    "text": "Modeling goals\nModels serve one of three main purposes:\n\nPrediction: predict new data before it is observed.\nInference: make conclusions about a larger population than the observed data.\nDescription: less common, but sometimes models provide a convenient description of observed data.\n\n\nWe probably wouldn’t use a univariate model to make specific predictions, but we could for instance estimate the probability that over 40 meteorites (rarely observed) hit earth any given day."
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#models-youve-seen-already",
    "href": "docs/Lectures/week7-lse.html#models-youve-seen-already",
    "title": "Modeling concepts; least squares",
    "section": "Models you’ve seen already",
    "text": "Models you’ve seen already\nThe exploratory analysis techniques you’ve seen are actually very flexible models often used for descriptive purposes.\n\nKernel density estimates are models for univariate data\nLOESS curves are models for trends in bivariate data\nPrincipal components are models for correlation structures\n\n\nIt’s a little tricky to see, but these correspond to classes of probability distributions rather than specific ones. That’s why they’re so flexible."
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#its-okay-not-to-model-data",
    "href": "docs/Lectures/week7-lse.html#its-okay-not-to-model-data",
    "title": "Modeling concepts; least squares",
    "section": "It’s okay not to model data",
    "text": "It’s okay not to model data\nThere are a lot of situations when modeling simply isn’t appropriate or feasible.\nSketchy sampling: every statistical model makes some assumptions about the data collection process. These don’t always need to hold exactly, but models could be untenable if:\n\nthe way data were collected is highly opaque or nonsystematic\nthe sampling design or measurement procedures have serious flaws or inconsistencies\n\n\nSparse data: model fitting is sensitive to the specific dataset observed, and may be unreliable if it’s too sensitive. This commonly arises when:\n\nthe model has a lot of parameters\nthe dataset contains few observations (relative to the number of parameters)"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#comment",
    "href": "docs/Lectures/week7-lse.html#comment",
    "title": "Modeling concepts; least squares",
    "section": "Comment",
    "text": "Comment\n\nThese univariate models usually don’t occur to us as models in the fullest sense, because:\n\nNo deterministic structure\nAll variation is random\n\n\nFor this reason you can’t do much with them, so they seem a bit uninteresting."
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#constructing-more-interesting-models",
    "href": "docs/Lectures/week7-lse.html#constructing-more-interesting-models",
    "title": "Modeling concepts; least squares",
    "section": "Constructing more interesting models",
    "text": "Constructing more interesting models\nMany models also involve an interesting deterministic component.\n\nA frequent strategy is to form a regression model:\n\nassume a distributional form for a quantity of interest\n‘regress’ the mean or other parameters of the distribution ‘on’ other variables – i.e., express the former as a function of the latter\n\n\n\nFor instance:\n\n\\(\\#\\text{meteors each day} \\sim \\text{poisson}(\\lambda)\\)\n\\(\\lambda = g(\\text{month})\\)"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#linear-models",
    "href": "docs/Lectures/week7-lse.html#linear-models",
    "title": "Modeling concepts; least squares",
    "section": "Linear models",
    "text": "Linear models\nA linear regression model is one in which the deterministic component is linear: the mean of a variable of interest is a linear function of one or more other variables.\nFor instance:\n\nYou may not have realized it at the time, but those lines are simple linear regression models: they describe the mean gaps as linear functions of log median income."
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#remarks-on-terminology",
    "href": "docs/Lectures/week7-lse.html#remarks-on-terminology",
    "title": "Modeling concepts; least squares",
    "section": "Remarks on terminology",
    "text": "Remarks on terminology\n\nA linear regression model is simple if it involves only one variable of interest \\(Y\\) and one additional variable \\(X\\)\n\n\\(Y\\) is the ‘response’, ‘dependent’, or ‘endogenous’ variable\n\\(X\\) is the ‘explanatory’, ‘independent’, or ‘exogenous’ variable\n\nThe multiple linear regression model involves multiple explanatory variables \\(X_1, X_2, \\dots\\)\nMultivariate linear regression models involve multiple responses \\(Y_1, Y_2, \\dots\\)\n“Linear regression model” is usually taken to mean a model in which \\(Y\\) is assumed to be normal, given \\(X\\)"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#applications-of-linear-models",
    "href": "docs/Lectures/week7-lse.html#applications-of-linear-models",
    "title": "Modeling concepts; least squares",
    "section": "Applications of linear models",
    "text": "Applications of linear models\nLinear models can be used for prediction, inference, or both.\n\nPredict the gender achievement gaps for a new district based on median income in the district.\nQuantify the association between median income and achievement gaps.\n\n\nWe’re going to talk in detail about the model itself:\n\ndefinition\nestimation\nassumptions"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#data-setting",
    "href": "docs/Lectures/week7-lse.html#data-setting",
    "title": "Modeling concepts; least squares",
    "section": "Data setting",
    "text": "Data setting\nLet’s first introduce the kind of data that the simple linear model describes.\n\nThere are two variables, \\(X\\) and \\(Y\\).\nThe data values are \\(n\\) observations of these two variables: \\[\n(x_1, y_1), \\dots, (x_n, y_n)\n\\]\n\n\nThe notation in tuples indicates the pairing of the values when measured on the same observational unit. If it helps, think of them as rows of an \\(n\\times 2\\) dataframe:\n\n\n\n\\(X\\)\n\\(Y\\)\n\n\n\n\n\\(x_1\\)\n\\(y_1\\)\n\n\n\\(x_2\\)\n\\(y_2\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(x_n\\)\n\\(y_n\\)"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#data-setting-1",
    "href": "docs/Lectures/week7-lse.html#data-setting-1",
    "title": "Modeling concepts; least squares",
    "section": "Data setting",
    "text": "Data setting\nThe notation above is just a mathematical description of data that looks like this:\n\nIn our notation, \\(X\\) would represent log median income, and \\(Y\\) would represent the math gap."
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#data-setting-2",
    "href": "docs/Lectures/week7-lse.html#data-setting-2",
    "title": "Modeling concepts; least squares",
    "section": "Data setting",
    "text": "Data setting\nThe example SEDA data in tabular form are:\n\n\nCode\n# import grade-aggregated seda data from hw2\nseda = pd.read_csv('data/seda.csv')\n\n# filter to math and remove NaNs\nregdata = seda[seda.subject == 'math'].dropna().drop(columns = 'subject').set_index('id')\n\n# dimensions\nn, p = regdata.shape\n\n# print\nregdata.head(3)\n\n\n\n\n\n\n\n\n\nlog_income\ngap\n\n\nid\n\n\n\n\n\n\n600001\n11.392048\n-0.562855\n\n\n600006\n11.607236\n0.061163\n\n\n600011\n10.704570\n-0.015417\n\n\n\n\n\n\n\n\nThe tuples would be: \\[\n(\\text{log_income}_1, \\text{gap}_1)\\;,\\; (\\text{log_income}_2, \\text{gap}_2)\\;,\\; \\dots\\;,\\; (\\text{log_income}_{625}, \\text{gap}_{625})\n\\]\n\n\nOr more specifically: \\[\n(11.392, -0.563)\\;,\\; (11.607, 0.061)\\;,\\; \\dots\\;,\\; (11.229, -0.040)\n\\]"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#lines-and-data",
    "href": "docs/Lectures/week7-lse.html#lines-and-data",
    "title": "Modeling concepts; least squares",
    "section": "Lines and data",
    "text": "Lines and data\nA line in slope-intercept form is given by the equation: \\[\ny = ax + b\n\\]\n\nData values never fall exactly on a line. So in general for every \\(a, b\\): \\[\ny_i \\neq a x_i + b\n\\]\n\n\nBut we can describe any dataset as a line and a ‘residual’: \\[\ny_i = \\underbrace{a x_i + b}_\\text{line} \\underbrace{+\\;e_i}_\\text{residual}\n\\]"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#lines-and-data-1",
    "href": "docs/Lectures/week7-lse.html#lines-and-data-1",
    "title": "Modeling concepts; least squares",
    "section": "Lines and data",
    "text": "Lines and data\nHere’s a picture:\n\n\nEach residual is simply the vertical distance of a value of \\(Y\\) from the line: \\[\n\\color{grey}{e_i} = \\color{blue}{y_i} - \\color{red}{(a x_i + b)}\n\\]"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#many-possible-lines",
    "href": "docs/Lectures/week7-lse.html#many-possible-lines",
    "title": "Modeling concepts; least squares",
    "section": "Many possible lines",
    "text": "Many possible lines\nThis makes it possible to express \\(Y\\) as a linear function of \\(X\\).\n\nHowever, the mathematical description is somewhat tautological, since for any \\(a, b\\), there are residuals \\(e_1, \\dots, e_n\\) such that \\[\ny_i = a x_i + b + e_i\n\\]\n\n\nIn other words, there are infinitely many possible lines. So which values of \\(a\\) and \\(b\\) should be chosen for a given set of data values?"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#the-least-squares-line",
    "href": "docs/Lectures/week7-lse.html#the-least-squares-line",
    "title": "Modeling concepts; least squares",
    "section": "The least squares line",
    "text": "The least squares line\nA sensible criterion is to find the line for which:\n\nthe average residual \\(\\bar{e}\\) is zero; and\nthe residuals vary the least.\n\n\nIf \\(\\bar{e} = 0\\), then the residual variance is proportional to the sum of squared residuals: \\[\n\\frac{1}{n - 1}\\sum_{i = 1}^n (e_i - \\bar{e})^2 = \\frac{1}{n - 1}\\sum_{i = 1}^n e_i^2\n\\]\n\n\nSo the values of \\(a, b\\) that minimize \\(\\sum_i e_i\\) give the ‘best’ line (in one sense of the word ‘best’). This method is known as least squares. \\[\n(a^*, b^*) = \\arg\\min_{(a, b)}\\left\\{\\sum_{i = 1}^n \\underbrace{\\left(y_i - (a x_i + b)\\right)^2}_{e_i^2}\\right\\}\n\\]"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#deriving-the-least-squares-line",
    "href": "docs/Lectures/week7-lse.html#deriving-the-least-squares-line",
    "title": "Modeling concepts; least squares",
    "section": "Deriving the least squares line",
    "text": "Deriving the least squares line\nLet’s try doing the derivation using univariate calculus. Note: \\(e_i = y_i - a - b x_i\\).\n\\[\n\\frac{d}{da} \\sum_i e_i^2 = \\cdots \\hskip{7in}\n\\]\n\\[\n\\frac{d}{db} \\sum_i e_i^2 = \\cdots \\hskip{7in}\n\\]\nIf \\(\\frac{d}{da} \\sum_i e_i^2 = 0\\) and \\(\\frac{d}{db} \\sum_i e_i^2 = 0\\) then…"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#deriving-the-least-squares-line-1",
    "href": "docs/Lectures/week7-lse.html#deriving-the-least-squares-line-1",
    "title": "Modeling concepts; least squares",
    "section": "Deriving the least squares line",
    "text": "Deriving the least squares line\nAlternatively, the model can be written in matrix form as \\(\\mathbf{y} = \\mathbf{Xa} + \\mathbf{e}\\), where:\n\\[\n\\underbrace{\\left[\\begin{array}{c} y_1 \\\\\\vdots\\\\ y_n \\end{array}\\right]}_{\\mathbf{y}}\n    = \\underbrace{\\left[\\begin{array}{cc}\n        1 & x_1 \\\\\n        \\vdots & \\vdots \\\\\n        1 & x_n\n        \\end{array}\\right]}_{\\mathbf{X}}\n      \\underbrace{\\left[\\begin{array}{c} a \\\\ b \\end{array}\\right]}_{\\mathbf{a}}\n      + \\underbrace{\\left[\\begin{array}{c} e_1 \\\\\\vdots\\\\ e_n \\end{array}\\right]}_{\\mathbf{e}}\n\\]\n\nThen, the sum of squared residuals is: \\[\n\\mathbf{e'e} = (\\mathbf{y} - \\mathbf{Xa})'(\\mathbf{y} - \\mathbf{Xa})\n\\]\n\n\nUsing vector calculus, one can show that: \\[\n\\nabla_\\mathbf{a} \\mathbf{e'e} = 0 \\quad\\Longrightarrow\\quad 2\\mathbf{X'y} = 2\\mathbf{X'Xa} \\quad\\Longrightarrow\\quad \\mathbf{a} = (\\mathbf{X'X})^{-1}\\mathbf{X'y}\n\\] And that this is a minimum."
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#calculating-the-least-squares-line",
    "href": "docs/Lectures/week7-lse.html#calculating-the-least-squares-line",
    "title": "Modeling concepts; least squares",
    "section": "Calculating the least squares line",
    "text": "Calculating the least squares line\nThe solution \\(\\mathbf{a} = (\\mathbf{X'X})^{-1}\\mathbf{X'y}\\) is known as the ordinarly least squares (OLS) estimate.\nThe sklearn implementation looks like this:\n\n# save explanatory variable and response variable separately as arrays\nx = regdata.log_income.values[:, np.newaxis]\ny = regdata.gap.values\n\n# configure regression module and fit model\nslr = LinearRegression()\nslr.fit(x, y)\n\n# store estimates\nprint('slope: ', slr.coef_[0])\nprint('intercept: ', slr.intercept_)\n\nslope:  0.12105696076155241\nintercept:  -1.3561699570330028"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#calculating-the-least-squares-line-1",
    "href": "docs/Lectures/week7-lse.html#calculating-the-least-squares-line-1",
    "title": "Modeling concepts; least squares",
    "section": "Calculating the least squares line",
    "text": "Calculating the least squares line\nThe solution \\(\\mathbf{a} = (\\mathbf{X'X})^{-1}\\mathbf{X'y}\\) is known as the ordinarly least squares (OLS) estimate.\nThe statsmodels implementation looks like this:\n\n# add a column of ones (intercept)\nx_aug = sm.tools.add_constant(x)\n\n# fit model\nslr = sm.OLS(endog = y, exog = x_aug)\nslr_fit = slr.fit()\n\n# return estimates\nprint('estimates: ', slr_fit.params)\n\nestimates:  [-1.35616996  0.12105696]\n\n\n\nNote a constant has to be added to x. Why?"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#calculating-the-least-squares-line-2",
    "href": "docs/Lectures/week7-lse.html#calculating-the-least-squares-line-2",
    "title": "Modeling concepts; least squares",
    "section": "Calculating the least squares line",
    "text": "Calculating the least squares line\nAlternatively, statsmodels also has a wrapper around sm.OLS that allows models to be fit based on a dataframe and column names (much like lm() in R):\n\nimport statsmodels.formula.api as smf\n\n# fit model based on dataframe and formula\nslr = smf.ols(formula = 'gap ~ log_income', data = regdata)\nslr_fit = slr.fit()\n\n# return estimates\nslr_fit.params\n\nIntercept    -1.356170\nlog_income    0.121057\ndtype: float64"
  },
  {
    "objectID": "docs/Lectures/week7-lse.html#calculating-the-least-squares-line-3",
    "href": "docs/Lectures/week7-lse.html#calculating-the-least-squares-line-3",
    "title": "Modeling concepts; least squares",
    "section": "Calculating the least squares line",
    "text": "Calculating the least squares line\nWe can check the calculations by computing the closed-form expression:\n\n# ols solution, by hand\nx_mx = np.vstack([np.repeat(1, len(x)), x[:, 0]]).transpose() # X\nxtx = x_mx.transpose().dot(x_mx) # X'X\nxtx_inv = np.linalg.inv(xtx) # (X'X)^{-1}\nxtx_inv.dot(x_mx.transpose()).dot(y) # (X'X)^{-1} X'y\n\narray([-1.35616996,  0.12105696])"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#from-last-time",
    "href": "docs/Lectures/week7-slr.html#from-last-time",
    "title": "Simple linear regression",
    "section": "From last time",
    "text": "From last time\n\nstatistical models are probabilistic represenations of data generating processes\n\nsome randomness must be present (usually from sampling) for this to be sensible\n\nregression models relate a variable of interest to one or more other ‘explanatory’ variables and consist of:\n\na distribution for the variable of interest\nexpression(s) for the distribution’s parameters in terms of the explanatory variable(s)\n\nfor linear regression, the mean of the response is a linear function of the explanatory variable(s)\n\n‘simple’ if one explanatory variable\n‘multiple’ if many explanatory variables\n‘multivariate’ if many variables of interest"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#from-last-time-1",
    "href": "docs/Lectures/week7-slr.html#from-last-time-1",
    "title": "Simple linear regression",
    "section": "From last time",
    "text": "From last time\nA simple linear model is \\(y_i = a x_i + b + e_i\\) where \\(e_i\\) are ‘residuals’ – leftover quantities.\n\nThe least squares line minimizes the sum of squared residuals \\(\\sum_i e_i^2\\) – i.e., the residual variance, assuming the line passes through the mean.\n\\[\n\\begin{align*}\nb &= \\frac{S_x}{S_y} \\text{corr}(x, y) \\\\\na &= \\bar{y} - b\\bar{x}\n\\end{align*}\n\\]"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#visualizing-the-line",
    "href": "docs/Lectures/week7-slr.html#visualizing-the-line",
    "title": "Simple linear regression",
    "section": "Visualizing the line",
    "text": "Visualizing the line\nVerifying that the line plotted on a prediction grid (left) matches the regression transform from Altair (right):"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#not-a-statistical-model-yet",
    "href": "docs/Lectures/week7-slr.html#not-a-statistical-model-yet",
    "title": "Simple linear regression",
    "section": "Not a statistical model, yet",
    "text": "Not a statistical model, yet\nThe least squares line is simply an algebraic transformation of the data – technically, a projection.\n\nThis is not yet a statistical model according to our definition, since there is no probability distribution involved.\n\n\nWe can change that by considering the residuals to be random."
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#residual-distribution",
    "href": "docs/Lectures/week7-slr.html#residual-distribution",
    "title": "Simple linear regression",
    "section": "Residual distribution",
    "text": "Residual distribution\nHave a look at the histogram of the residuals (with a KDE curve):\n\n\nDoes this look like any probability density function you encountered in 120A?"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#residual-distribution-1",
    "href": "docs/Lectures/week7-slr.html#residual-distribution-1",
    "title": "Simple linear regression",
    "section": "Residual distribution",
    "text": "Residual distribution\nThe residual distribution is pretty well-approximated by the normal or Gaussian distribution:"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#the-error-model",
    "href": "docs/Lectures/week7-slr.html#the-error-model",
    "title": "Simple linear regression",
    "section": "The error model",
    "text": "The error model\nThis phenomenon – nearly normal residuals – is pretty common in practice.\n\nSo a standard distributional model for the residuals is that they are independent normal random variables. This is written as:\n\\[\ne_i \\stackrel{iid}{\\sim} N\\left(0, \\sigma^2\\right)\n\\]\n\n\nThis is an important modification because it induces a probability distribution on \\(y_i\\)."
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#the-simple-linear-regression-model",
    "href": "docs/Lectures/week7-slr.html#the-simple-linear-regression-model",
    "title": "Simple linear regression",
    "section": "The simple linear regression model",
    "text": "The simple linear regression model\nNow we’re in a position to state the simple linear regression model:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad\n\\begin{cases}\ni = 1, \\dots, n \\\\\n\\epsilon_i \\sim N\\left(0,\\sigma^2\\right)\n\\end{cases}\n\\]\n\n\\(y_i\\) is the response variable\n\\(x_i\\) is the explanatory variable\n\\(\\epsilon_i\\) is the random error\n\\(\\beta_0, \\beta_1, \\sigma^2\\) are the model parameters\n\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the coefficient\n\\(\\sigma^2\\) is the error variance"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#properties-of-the-normal-distribution",
    "href": "docs/Lectures/week7-slr.html#properties-of-the-normal-distribution",
    "title": "Simple linear regression",
    "section": "Properties of the normal distribution",
    "text": "Properties of the normal distribution\nAs a refresher from 120A, if \\(X \\sim N(\\mu, \\sigma^2)\\) then:\n\n(Mean) \\(\\mathbb{E}X = \\mu\\)\n(Variance) \\(\\text{var}X = \\sigma^2\\)\n(Linearity) For constants \\(a, b\\), \\(aX + b \\sim N(a\\mu + b, a^2\\sigma^2)\\)"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#model-implications",
    "href": "docs/Lectures/week7-slr.html#model-implications",
    "title": "Simple linear regression",
    "section": "Model implications",
    "text": "Model implications\nTreating the error term as random has a number of implications that follow from the properties of the normal distribution:\n\n(Normality) The response is a normal random variable: \\(y_i \\sim N\\left(\\beta_0 + \\beta_1 x_i, \\sigma^2\\right)\\)\n(Linearity) The mean response is linear in \\(X\\): \\(\\mathbb{E}y_i = \\beta_0 + \\beta_1 x_i\\)\n(Constant variance) The response has variance: \\(\\text{var}y_i = \\sigma^2\\)\n(Independence) The observations are independent (because the errors are): \\(y_i \\perp y_j\\)\n\n\nThese are the assumptions of the simple linear regression model.\n\n\nAside: other error distributions, or conditions that don’t assume a specific distribution, are possible."
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#estimates",
    "href": "docs/Lectures/week7-slr.html#estimates",
    "title": "Simple linear regression",
    "section": "Estimates",
    "text": "Estimates\nYou’ve already seen how to compute the least squares estimates of \\(\\beta_0, \\beta_1\\) – these are no different when the errors are random.\n\nEstimates are typically denoted by the corresponding paramater with a hat:\n\\[\n\\begin{align*}\n\\hat{\\beta}_1\n    &= \\frac{\n        \\sum_i (x_i - \\bar{x})(y_i - \\bar{y})\n        }{\n        \\sum_i (x_i - \\bar{x})^2\n        } \\\\\n\\hat{\\beta}_0\n    &= \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\end{align*}\n\\]\n\n\nAn estimate of the error variance is:\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{n - 2} \\sum_{i = 1}^n \\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\right)^2\n\\]"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#estimates-matrix-form",
    "href": "docs/Lectures/week7-slr.html#estimates-matrix-form",
    "title": "Simple linear regression",
    "section": "Estimates (matrix form)",
    "text": "Estimates (matrix form)\nThese estimates can be expressed in matrix form as:\n\\[\n\\begin{align*}\n\\hat{\\beta} &= (\\mathbf{X'X})^{-1}\\mathbf{X'y} \\\\\n\\hat{\\sigma}^2\n    &= \\frac{1}{n - 2}\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)'\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#fitted-values-and-residuals",
    "href": "docs/Lectures/week7-slr.html#fitted-values-and-residuals",
    "title": "Simple linear regression",
    "section": "Fitted values and residuals",
    "text": "Fitted values and residuals\nThe projections of the data points onto the line are the estimated values of the response variable for each data point.\nThese are known as fitted values and denoted \\(\\hat{y}_i\\):\n\\[\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n\\]\n\nThen, the differences between observed and fitted values give the model residuals:\n\\[\ne_i = y_i - \\hat{y}_i\n\\]"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#computations",
    "href": "docs/Lectures/week7-slr.html#computations",
    "title": "Simple linear regression",
    "section": "Computations",
    "text": "Computations\n\n# retrieve data\nx = sm.tools.add_constant(regdata.log_income.values)\ny = regdata.gap.values\n\n# fit regression model\nslr = sm.OLS(endog = y, exog = x)\n\n# estimates\nbeta_hat = slr.fit().params\n\n# fitted values\nfitted = slr.fit().fittedvalues\n\n# residuals\nresids = slr.fit().resid\n\n# error variance estimate\nsigmasq_hat = slr.fit().scale\n\nprint('coefficient estimates: ', beta_hat)\nprint('error variance estimate: ', sigmasq_hat)\n\ncoefficient estimates:  [-1.35616996  0.12105696]\nerror variance estimate:  0.013171170616137325"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#parameter-interpretations-intercept",
    "href": "docs/Lectures/week7-slr.html#parameter-interpretations-intercept",
    "title": "Simple linear regression",
    "section": "Parameter interpretations: intercept",
    "text": "Parameter interpretations: intercept\nThe intercept \\(\\beta_0\\) represents the mean response \\(\\mathbb{E}y_i\\) when \\(x_i = 0\\).\n\nIn the SEDA example:\n\nFor districts with a log median income of 0, the mean achievement gap between boys and girls is estimated to be -1.356 standard deviations from the national average.\n\n\n\nNot incorrect, but awkward:\n\nlog median income is not a natural quantity\nthe sign is confusing"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#parameter-interpretations-intercept-1",
    "href": "docs/Lectures/week7-slr.html#parameter-interpretations-intercept-1",
    "title": "Simple linear regression",
    "section": "Parameter interpretations: intercept",
    "text": "Parameter interpretations: intercept\nBetter:\n\nFor school districts with a median income of 1 dollar, the mean achievement gap is estimated to favor girls by 1.356 standard deviations from the national average.\n\n\nCheck your understanding:\n\nwhy 1 dollar and not 0 dollars?\nwhy not -1.356?\n\nNot of particular interest here because no districts have a median income of 1 USD."
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#parameter-interpretations-slope",
    "href": "docs/Lectures/week7-slr.html#parameter-interpretations-slope",
    "title": "Simple linear regression",
    "section": "Parameter interpretations: slope",
    "text": "Parameter interpretations: slope\nThe slope \\(\\beta_1\\) represents the change in mean response \\(\\mathbb{E}y_i\\) per unit change in \\(x_i\\).\n\nIn the SEDA example:\n\nEach increase of log median income by 1 is associated with an estimated increase in mean achievement gap of 0.121 standard deviations from the national average in favor of boys.\n\n\n\nNot incorrect, but a bit awkward – how much is a change in log median income of 1 unit?"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#parameter-interpretations-slope-1",
    "href": "docs/Lectures/week7-slr.html#parameter-interpretations-slope-1",
    "title": "Simple linear regression",
    "section": "Parameter interpretations: slope",
    "text": "Parameter interpretations: slope\nBetter:\n\nEvery doubling of median income is associated with an estimated increase in the mean achievement gap of 0.084 standard deviations from the national average in favor of boys.\n\n\nWhy doubling??\nHint: \\(\\hat{\\beta}_1\\log (2x) = \\hat{\\beta}_1\\log x + \\hat{\\beta}_1 \\log 2\\)"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#parameter-interpretations-error-variance",
    "href": "docs/Lectures/week7-slr.html#parameter-interpretations-error-variance",
    "title": "Simple linear regression",
    "section": "Parameter interpretations: error variance",
    "text": "Parameter interpretations: error variance\nThe error variance \\(\\sigma^2\\) represents the variability in the response \\(y\\) after accounting for the explanatory variable \\(x\\).\n\nIn the SEDA example:\n\nAfter adjusting for log median income, the gender achievement gap varies among districts by an estimated 0.11 standard deviations from the national average.\n\n\n\nNote that \\(\\hat{\\sigma}\\) is reported for interpretation on the original scale, rather than \\(\\hat{\\sigma}^2\\)."
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#parameter-interpretations-error-variance-1",
    "href": "docs/Lectures/week7-slr.html#parameter-interpretations-error-variance-1",
    "title": "Simple linear regression",
    "section": "Parameter interpretations: error variance",
    "text": "Parameter interpretations: error variance\nCompare the estimated ‘raw’ variance in gender gap with the estimated residual variance after accounting for log median income:\n\n\nCode\nprint('raw variance: ', y.var(ddof = 1))\nprint('estimated residual variance: ', sigmasq_hat)\n\n\nraw variance:  0.015355417268162816\nestimated residual variance:  0.013171170616137325\n\n\n\nThe estimated variability in achievement gap diminishes a little after adjusting for log median income. The relative reduction is:\n\\[\n\\frac{\\hat{\\sigma}^2_\\text{raw} - \\hat{\\sigma}^2}{\\hat{\\sigma}^2_\\text{raw}}\n\\]\n\n\nIn the SEDA example, the reduction was about 14%:\n\n\nCode\nprint('relative reduction in variance: ',\n    (y.var(ddof = 1) - sigmasq_hat)/y.var(ddof = 1)\n)\n\n\nrelative reduction in variance:  0.14224599787035444"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#parameter-interpretations-variance",
    "href": "docs/Lectures/week7-slr.html#parameter-interpretations-variance",
    "title": "Simple linear regression",
    "section": "Parameter interpretations: variance",
    "text": "Parameter interpretations: variance\nA closely related quantity is the R-squared statistic, which simply adjusts the denominator of the error variance estimate:\n\\[\n\\frac{\\hat{\\sigma}^2_\\text{raw} - \\frac{n - 2}{n - 1}\\hat{\\sigma}^2}{\\hat{\\sigma}^2_\\text{raw}}\n\\]\n\nused as a measure of fit\ninterpreted as the proportion of variation in the response explained by the model"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#general-parameter-interpretations",
    "href": "docs/Lectures/week7-slr.html#general-parameter-interpretations",
    "title": "Simple linear regression",
    "section": "General parameter interpretations",
    "text": "General parameter interpretations\nThere is some general language for interpreting the parameter estimates:\n\n(Intercept) When [\\(x_i = 0\\)] the mean [response variable] is estimated to be [\\(\\hat{\\beta}_0\\) units].\n(Slope) Every [one-unit increase in \\(x_i\\)] is associated with an estimated change in mean [response variable] of [\\(\\hat{\\beta}_1\\) units].\n(Error variance) After adjusting for [explanatory variable], the remaining variability in [response variable] is an estimated [\\(\\hat{\\sigma}\\) units] about the mean.\n\n\nYou can use this standard language as a formulaic template for interpreting estimated parameters."
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#centering-the-explanatory-variable",
    "href": "docs/Lectures/week7-slr.html#centering-the-explanatory-variable",
    "title": "Simple linear regression",
    "section": "Centering the explanatory variable",
    "text": "Centering the explanatory variable\nIf we want the intercept to be meaningful, we could center the explanatory variable and instead fit:\n\\[\ny_i = \\beta_0 + \\beta_1 (x_i - \\bar{x}) + \\epsilon_i\n\\]\n\n\nCode\n# center log median income\nlog_income_ctr = (regdata.log_income - regdata.log_income.mean()).values\n\n# form x matrix\nx_ctr = sm.tools.add_constant(log_income_ctr)\n\n# refit model\nslr_ctr = sm.OLS(endog = y, exog = x_ctr)\nbeta_hat_ctr = slr_ctr.fit().params\n\n# display parameter estimates\nprint('coefficient estimates: ', beta_hat_ctr)\nprint('error variance estimate: ', slr_ctr.fit().scale)\n\n\ncoefficient estimates:  [-0.02105724  0.12105696]\nerror variance estimate:  0.013171170616137325\n\n\n\n\nFor a district with average log median income, the estimated achievement gap favors boys by 0.021 standard deviations from the national average.\n\n\nnote that this is just a location shift so other estimates are unchanged\ncan recover the original intercept estimate as \\(\\hat{\\beta}_0 - \\hat{\\beta}_1 \\bar{x}\\)\n\n\n\n\nbeta_hat_ctr[0] - beta_hat_ctr[1]*regdata.log_income.mean()\n\n-1.356169957033003"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#other-transformations",
    "href": "docs/Lectures/week7-slr.html#other-transformations",
    "title": "Simple linear regression",
    "section": "Other transformations",
    "text": "Other transformations\nWe could seek to adjust the model so that the intercept is interpreted as the gap at the district with the smallest median income:\n\\[\ny_i = \\beta_0 + \\beta_1 \\log\\left(x_i - x_{(1)} + 1 \\right), \\quad x_i: \\text{median income for district } i\n\\]\n\nBut this changes the meaning of the other model terms:\n\n\\(\\beta_1\\) represents the change in mean gap associated with multiplicative changes in the amount by which a district’s median income exceeds that of the poorest district\n\\(\\sigma^2\\) is the variability of the gap after adjusting for the log of the difference in median income from the median income of the poorest district"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#other-transformations-1",
    "href": "docs/Lectures/week7-slr.html#other-transformations-1",
    "title": "Simple linear regression",
    "section": "Other transformations",
    "text": "Other transformations\nUnsurprisingly, estimates are not invariant under arbitrary transformations, so if the meanings of the other parameters change, then so do the estimates:\n\n# center log median income\nincome = np.exp(regdata.log_income) \nincome_shifted = income - income.min()\nlog_income_shifted = np.log(income_shifted + 1)\n\n# form x matrix\nx_shifted = sm.tools.add_constant(log_income_shifted)\n\n# refit model\nslr_shifted = sm.OLS(endog = y, exog = x_shifted)\nbeta_hat_shifted = slr_shifted.fit().params\n\n# display parameter estimates\nprint('coefficient estimates: ', beta_hat_shifted)\nprint('error variance estimate: ', slr_shifted.fit().scale)\n\ncoefficient estimates:  const        -0.544166\nlog_income    0.049851\ndtype: float64\nerror variance estimate:  0.013904979364206382\n\n\n\nNote also it’s not possible to express the old parameters as functions of the new parameters; this is a fundamentally different model."
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#uncertainty-quantification",
    "href": "docs/Lectures/week7-slr.html#uncertainty-quantification",
    "title": "Simple linear regression",
    "section": "Uncertainty quantification",
    "text": "Uncertainty quantification\nA great benefit of the simple linear regression model relative to a best-fit line is that the error variance estimate allows for uncertainty quantification.\n\nWhat that means is that one can describe precisely:\n\nvariation in the estimates (i.e., estimated model reliability);\nvariation in predictions made using the estimated model (i.e., predictive reliability)."
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#understanding-variation-in-estimates",
    "href": "docs/Lectures/week7-slr.html#understanding-variation-in-estimates",
    "title": "Simple linear regression",
    "section": "Understanding variation in estimates",
    "text": "Understanding variation in estimates\nWhat would happen to the estimates if they were computed from a different sample?\n\nWe can explore this idea a little by calculating least squares estimates from several distinct subsamples of the dataset.\n\n\n\nThe lines are pretty similar, but they change a bit from subsample to subsample."
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#variance-of-least-squares",
    "href": "docs/Lectures/week7-slr.html#variance-of-least-squares",
    "title": "Simple linear regression",
    "section": "Variance of least squares",
    "text": "Variance of least squares\n\nHow much should one expect the estimates to change depending on the data they are fit to?\n\n\nIt can be shown that the variances and covariance of the estimates are:\n\\[\n\\left[\\begin{array}{cc}\n    \\text{var}\\hat{\\beta}_0 & \\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right) \\\\\n    \\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right) &\\text{var}\\hat{\\beta}_1\n    \\end{array}\\right]\n   = \\sigma^2 \\left(\\mathbf{X'X}\\right)^{-1}\n\\]\n\n\nBear in mind that the randomness comes from the \\(\\epsilon_i\\) model term.\n\nthese quantify how much the parameters vary across collections of \\(y_i\\)’s measured at exactly the same values of \\(x_i\\)\nthese are not variances of the parameters; \\(\\beta_0\\) and \\(\\beta_1\\) are constants, i.e., not random\nthey are also not variances of the estimates – e.g., \\(0.121\\) is yet another constant"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#standard-errors",
    "href": "docs/Lectures/week7-slr.html#standard-errors",
    "title": "Simple linear regression",
    "section": "Standard errors",
    "text": "Standard errors\nSo the variances can be estimated by plugging in \\(\\color{red}{\\hat{\\sigma}^2}\\) for \\(\\sigma\\) in the variance-covariance matrix from the previous slide.\n\nThe estimated standard deviations are known as standard errors:\n\\[\n\\text{SE}(\\hat{\\beta}_0) = \\sqrt{\\color{red}{\\hat{\\sigma}^2}(\\mathbf{X'X})^{-1}_{11}} \\qquad\\text{and}\\qquad \\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\color{red}{\\hat{\\sigma}^2}(\\mathbf{X'X})^{-1}_{22}}\n\\]"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#computations-and-intepretations",
    "href": "docs/Lectures/week7-slr.html#computations-and-intepretations",
    "title": "Simple linear regression",
    "section": "Computations and intepretations",
    "text": "Computations and intepretations\nThe estimated variance-covariance of the least squares estimates is computed by the .cov_params() method:\n\nslr.fit().cov_params()\n\narray([[ 0.01708179, -0.00154692],\n       [-0.00154692,  0.00014026]])\n\n\n\nthe intercept estimate varies by an estimated \\(\\sqrt{0.0171} = 0.131\\) across datasets, with \\(x_i\\) fixed\nthe slope estimate varies by an estimated \\(\\sqrt{0.00014} = 0.0118\\) across datasets, with \\(x_i\\) fixed"
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#confidence-intervals",
    "href": "docs/Lectures/week7-slr.html#confidence-intervals",
    "title": "Simple linear regression",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nAbout 95% of the time, the true values \\(\\beta_0, \\beta_1\\) will be within 2SE of any particular estimates \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\).\n\nThe intervals \\(\\hat{\\beta}_j \\pm 2 SE(\\hat{\\beta}_j)\\) provide ranges of possible values for the true values \\(\\beta_j\\):\n\nslr.fit().conf_int()\n\narray([[-1.61283063, -1.09950928],\n       [ 0.09779946,  0.14431446]])\n\n\n\n\nThis supports two important inferences:\n\nwith 95% confidence, the intercept is estimated to be between -1.613 and -1.010\nwith 95% confidence, the slope is estimated to be between 0.098 and 0.144\n\n\n\nAnd the width of those intervals conveys a sense of the uncertainty associated with the estimates."
  },
  {
    "objectID": "docs/Lectures/week7-slr.html#visualizing-uncertainty",
    "href": "docs/Lectures/week7-slr.html#visualizing-uncertainty",
    "title": "Simple linear regression",
    "section": "Visualizing uncertainty",
    "text": "Visualizing uncertainty\nIt’s fairly common practice to add a band around the plotted line to indicate estimated variability.\n\n\n\n\n\nrepresents the uncertainty/variability of the parameter estimates, i.e., of the trend line\ndoes not represent the uncertainty/variability of the observations – note the spread of data is considerably broader than the uncertainty band"
  },
  {
    "objectID": "materials.html#week-6",
    "href": "materials.html#week-6",
    "title": "Materials",
    "section": "Week 6:",
    "text": "Week 6:\nReadings:\n\nBasics of prediction intervals\nMultiple regression\n\nMonday-Thursday[slides]\nLab6 [html], [zip], due on Monday 08/07."
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#announcements",
    "href": "docs/Lectures/week8-mlr.html#announcements",
    "title": "Multiple regression",
    "section": "Announcements",
    "text": "Announcements\nNo class Monday 5/29 due to Memorial Day.\n\nAssignments due Tuesday instead.\nLate deadline moved to Thursday.\nWednesday office hour cancelled on 5/31."
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#loose-ends",
    "href": "docs/Lectures/week8-mlr.html#loose-ends",
    "title": "Multiple regression",
    "section": "Loose ends",
    "text": "Loose ends\nThe standard measure of predictive accuracy in regression is mean square error (MSE):\n\\[\nMSE(y, \\hat{y}) = \\frac{1}{n}\\sum_i (y_i - \\hat{y}_i)^2\n\\]\n\nestimates expected squared error, \\(\\mathbb{E}(y - \\hat{y})^2\\)\nbiased (underestimate on average) if fitted values are used\nunbiased if new observations are used\n\n\nTo avoid bias, it is common practice to partition data into nonoverlapping subsets:\n\none used to fit the model (‘training’ partition)\nanother used to evaluate predictions (‘testing’ or ‘validation’ partition)"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#measuring-predictive-accuracy",
    "href": "docs/Lectures/week8-mlr.html#measuring-predictive-accuracy",
    "title": "Multiple regression",
    "section": "Measuring predictive accuracy",
    "text": "Measuring predictive accuracy\nPartition the data:\n\n# hold out 100 randomly selected rows\nnp.random.seed(51823)\nidx = np.random.choice(regdata.index.values, size = 100, replace = False).tolist()\ntest = regdata.loc[idx]\ntrain = regdata.drop(index = idx)\n\n\nFit to the training partition:\n\n# fit model to training subset\nx_train = sm.tools.add_constant(train.log_income)\ny_train = train.gap\nslr = sm.OLS(endog = y_train, exog = x_train)\n\n\n\nEvaluate on the test/validation partition:\n\n# compute predictions\nx_test = sm.tools.add_constant(test.log_income)\npreds = slr.fit().get_prediction(x_test)\ny_hat = preds.predicted_mean\n\n# mean square error\npred_errors = test.gap - y_hat\nmse = (pred_errors**2).mean()\nprint('root mean square error: ', np.sqrt(mse))\n\nroot mean square error:  0.14377667235816582"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#interpreting-mse",
    "href": "docs/Lectures/week8-mlr.html#interpreting-mse",
    "title": "Multiple regression",
    "section": "Interpreting MSE",
    "text": "Interpreting MSE\nInterpretation:\n\nThe model predictions vary about observed values with a standard deviation of 0.144 (SD of national average)."
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#dont-use-training-mse",
    "href": "docs/Lectures/week8-mlr.html#dont-use-training-mse",
    "title": "Multiple regression",
    "section": "(Don’t use) training MSE",
    "text": "(Don’t use) training MSE\nCompare with MSE computed using fitted values:\n\n\nCode\n# note, these are just model residuals\nfit_errors = train.gap - slr.fit().fittedvalues\n\n# training rmse\nprint('rmse on training partition: ',np.sqrt((fit_errors**2).mean()))\nprint('rmse on test partition: ', np.sqrt(mse))\n\n\nrmse on training partition:  0.10843963485562963\nrmse on test partition:  0.14377667235816582\n\n\n\ntraining MSE is overly ‘optimistic’ – smaller than the proper estimate\nwon’t always be the case, but will be an underestimate on average across samples\n\n\nNote also that this is simply the estimate of the error variance, rescaled by \\(\\frac{n - 2}{n}\\).\n\nn, p = train.shape\nnp.sqrt(slr.fit().scale*(n - 2)/n)\n\n0.10843963485562962\n\n\n\n\nSince the model is fit by minimizing this quantity, out-of-sample predictions are absolutely necessary to get a good sense of the predictive reliability."
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#multiple-regression",
    "href": "docs/Lectures/week8-mlr.html#multiple-regression",
    "title": "Multiple regression",
    "section": "Multiple regression",
    "text": "Multiple regression\nThe simple linear regression model has just one explanatory variable:\n\\[\n(\\text{SLR}) \\qquad\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\quad\\begin{cases} i = 1, \\dots, n \\\\\\epsilon_i \\sim N\\left(0,\\sigma^2\\right)\\end{cases}\n\\]\n\nThe multiple linear regression model is a direct extension of the simple linear model to \\(p - 1\\) variables \\(x_{i1}, \\dots, x_{i, p - 1}\\):\n\\[\n(\\text{MLR})\\qquad\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_{p - 1} x_{i, p - 1} + \\epsilon_i \\qquad\\begin{cases} \\epsilon_i \\sim N(0, \\sigma^2) \\\\ i = 1, \\dots, n\\end{cases}\n\\]\n\n\\(p\\) is the number of (mean) parameters\n\\(p = 2\\) is SLR\n\\(p &gt; 2\\) is MLR\nWhat’s \\(p = 1\\)??"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#the-model-in-matrix-form",
    "href": "docs/Lectures/week8-mlr.html#the-model-in-matrix-form",
    "title": "Multiple regression",
    "section": "The model in matrix form",
    "text": "The model in matrix form\nThe model in matrix form is \\(\\mathbf{y} = \\mathbf{X}\\beta + \\epsilon\\), where:\n\\[\n\\mathbf{y}: \\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array}\\right] \\; = \\;\n    \\mathbf{X}: \\left[\\begin{array}{cccc}\n        1 &x_{11} &\\cdots &x_{1, p - 1} \\\\\n        1 &x_{21} &\\cdots &x_{2, p - 1} \\\\\n        \\vdots &\\vdots &\\ddots &\\vdots \\\\\n        1 &x_{n1} &\\cdots &x_{n, p - 1}\n        \\end{array}\\right] \\; \\times \\;\n    \\beta: \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_{p - 1} \\end{array} \\right] \\; + \\;\n    \\epsilon: \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{array}\\right]\n\\]\n\nCarrying out the arithmetic on the right-hand side:\n\\[\n\\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array}\\right]_{\\;n \\times 1} \\quad = \\quad\n    \\left[\\begin{array}{c}\n        \\beta_0 + \\beta_1 x_{11} + \\cdots + \\beta_{p - 1} x_{1, p - 1} + \\epsilon_1 \\\\\n        \\beta_0 + \\beta_1 x_{21} + \\cdots + \\beta_{p - 1} x_{2, p - 1} + \\epsilon_2 \\\\\n        \\vdots \\\\\n        \\beta_0 + \\beta_1 x_{n1} + \\cdots + \\beta_{p - 1} x_{n, p - 1} + \\epsilon_n\n        \\end{array}\\right]_{\\;n \\times 1}\n\\]\n\n\nThis is exactly the model relationship as written before, enumerated for each \\(i\\)."
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#example",
    "href": "docs/Lectures/week8-mlr.html#example",
    "title": "Multiple regression",
    "section": "Example",
    "text": "Example\nLet’s consider the model you fit in lab:\n\\[\n(\\text{fertility rate})_i = \\beta_0 + \\beta_1 (\\text{HDI})_i + \\beta_2 (\\text{education})_i + \\epsilon_i\n\\]\n\nx_vars = fertility.loc[:, ['educ_expected_yrs_f', 'hdi']]\nx = sm.tools.add_constant(x_vars)\ny = fertility.fertility_total\n\nmlr = sm.OLS(endog = y, exog = x)"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#model-estimation",
    "href": "docs/Lectures/week8-mlr.html#model-estimation",
    "title": "Multiple regression",
    "section": "Model estimation",
    "text": "Model estimation\nEstimation and uncertainty quantification are exactly the same as in the simple linear model.\n\nThe least squares estimates are: \\[\n\\hat{\\beta} = \\left[\\begin{array}{c}\\hat{\\beta}_0 \\\\ \\vdots \\\\ \\hat{\\beta}_{p - 1} \\end{array}\\right] = \\left(\\mathbf{X'X}\\right)^{-1}\\mathbf{X'y}\n\\]\n\n# retrieve coefficient estimates\nmlr.fit().params\n\nconst                  7.960371\neduc_expected_yrs_f   -0.201996\nhdi                   -4.132623\ndtype: float64\n\n\n\n\nThis is the unique minimizer of the residual variance:\n\\[\n\\hat{\\beta} = \\text{argmin}_{\\beta \\in \\mathbb{R}^p}\\left\\{ (\\mathbf{y} - \\mathbf{X}\\beta)'(\\mathbf{y} - \\mathbf{X}\\beta)\\right\\}\n\\]"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#model-estimation-1",
    "href": "docs/Lectures/week8-mlr.html#model-estimation-1",
    "title": "Multiple regression",
    "section": "Model estimation",
    "text": "Model estimation\nAn estimate of the error variance is: \\[\n\\hat{\\sigma}^2\n    = \\frac{1}{n - p} \\sum_{i = 1}^n \\underbrace{\\left(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{i1} - \\cdots - \\hat{\\beta}_{p - 1}x_{i, p - 1}\\right)^2}_{i\\text{th squared model residual}}\n\\]\n\n# retrieve error variance estimate\nmlr.fit().scale\n\n0.3469089355991577\n\n\n\nThere are several alternative ways of writing the estimator \\(\\hat{\\sigma}^2\\).\n\n\\(\\frac{1}{n - p} \\sum_i (y_i - \\hat{y}_i)^2\\)\n\\(\\frac{1}{n - p} \\sum_i e_i^2\\)\n\\(\\frac{1}{n - p}\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)'\\left(\\mathbf{y} - \\mathbf{X}\\hat{\\beta}\\right)\\)\n\\(\\frac{RSS}{n - p}\\)"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#variability-of-estimates",
    "href": "docs/Lectures/week8-mlr.html#variability-of-estimates",
    "title": "Multiple regression",
    "section": "Variability of estimates",
    "text": "Variability of estimates\nThe variances and covariances of the coefficient estimates are found in exactly the same way as before, only now yield a \\(p \\times p\\) (instead of \\(2\\times 2\\)) matrix:\n\\[\n\\mathbf{V}\n= \\left[\\begin{array}{cccc}\n    \\text{var}\\hat{\\beta}_0\n        &\\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right)\n        &\\cdots &\\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_{p - 1}\\right) \\\\\n    \\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_1\\right)\n        &\\text{var}\\hat{\\beta}_1\n        &\\cdots &\\text{cov}\\left(\\hat{\\beta}_1, \\hat{\\beta}_{p - 1}\\right) \\\\\n    \\vdots &\\vdots &\\ddots &\\vdots \\\\\n    \\text{cov}\\left(\\hat{\\beta}_0, \\hat{\\beta}_{p - 1}\\right)\n        &\\text{cov}\\left(\\hat{\\beta}_1, \\hat{\\beta}_{p - 1}\\right)\n        &\\cdots\n        &\\text{var}\\hat{\\beta}_{p - 1}\n    \\end{array}\\right]\n   = \\sigma^2 \\left(\\mathbf{X'X}\\right)^{-1}\n\\]"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#variability-of-estimates-1",
    "href": "docs/Lectures/week8-mlr.html#variability-of-estimates-1",
    "title": "Multiple regression",
    "section": "Variability of estimates",
    "text": "Variability of estimates\nThis matrix is again estimated by plugging in \\(\\color{red}{\\hat{\\sigma}^2}\\) (the estimate) for \\(\\sigma^2\\): \\[\\hat{\\mathbf{V}}\n    = \\left[\\begin{array}{cccc}\n        \\color{blue}{\\hat{v}_{11}} & \\hat{v}_{12} & \\cdots & \\hat{v}_{1p} \\\\\n        \\hat{v}_{21} & \\color{blue}{\\hat{v}_{22}} & \\cdots & \\hat{v}_{2p} \\\\\n        \\vdots &\\vdots &\\ddots &\\vdots \\\\\n        \\hat{v}_{p1} & \\hat{v}_{p2} &\\cdots &\\color{blue}{\\hat{v}_{pp}} \\\\\n        \\end{array}\\right]\n    = \\color{red}{\\hat{\\sigma}^2}\\left(\\mathbf{X'X}\\right)^{-1}\n\\]\n\n# retrieve parameter variance-covariance estimate\nmlr.fit().cov_params()\n\n\n\n\n\n\n\n\nconst\neduc_expected_yrs_f\nhdi\n\n\n\n\nconst\n0.059995\n-0.001839\n-0.050256\n\n\neduc_expected_yrs_f\n-0.001839\n0.001780\n-0.025240\n\n\nhdi\n-0.050256\n-0.025240\n0.462611"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#standard-errors",
    "href": "docs/Lectures/week8-mlr.html#standard-errors",
    "title": "Multiple regression",
    "section": "Standard errors",
    "text": "Standard errors\nThe square roots of the diagonal elements give standard errors: \\[\n\\text{SE}(\\hat{\\beta}_0) = \\sqrt{\\color{blue}{\\hat{v}_{11}}}\n    \\;,\\quad\n    \\text{SE}(\\hat{\\beta}_1) = \\sqrt{\\color{blue}{\\hat{v}_{22}}}\n    \\;,\\quad \\cdots \\quad\n    \\text{SE}(\\hat{\\beta}_{p - 1}) = \\sqrt{\\color{blue}{\\hat{v}_{pp}}}\n\\]\n\nnp.sqrt(mlr.fit().cov_params().values.diagonal())\n\narray([0.24493942, 0.042194  , 0.68015545])"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#model-fit-summary",
    "href": "docs/Lectures/week8-mlr.html#model-fit-summary",
    "title": "Multiple regression",
    "section": "Model fit summary",
    "text": "Model fit summary\nJust as before, a table is helpful:\n\n\nCode\nrslt = mlr.fit()\ncoef_tbl = pd.DataFrame({'estimate': rslt.params.values,\n              'standard error': np.sqrt(rslt.cov_params().values.diagonal())},\n              index = x.columns)\ncoef_tbl.loc['error variance', 'estimate'] = rslt.scale\n\ncoef_tbl\n\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\nconst\n7.960371\n0.244939\n\n\neduc_expected_yrs_f\n-0.201996\n0.042194\n\n\nhdi\n-4.132623\n0.680155\n\n\nerror variance\n0.346909\nNaN"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#variance-explained",
    "href": "docs/Lectures/week8-mlr.html#variance-explained",
    "title": "Multiple regression",
    "section": "Variance explained",
    "text": "Variance explained\nThe \\(R^2\\) statistic – proportion of variance explained – is computed the same as before: \\[\n\\frac{\\hat{\\sigma}_\\text{raw}^2 - \\frac{n - 1}{n - p}\\hat{\\sigma}^2}{\\hat{\\sigma}_\\text{raw}^2}\n\\]\n\nrslt.rsquared\n\n0.7827796420988886"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#interpretation",
    "href": "docs/Lectures/week8-mlr.html#interpretation",
    "title": "Multiple regression",
    "section": "Interpretation",
    "text": "Interpretation\nWhile the computations are essentially unchanged, the presence of multiple predictors alters the interpretation of the parameters.\n\nIncrementing, say, HDI by one unit to get an estimated change assumes the other variable is held constant: \\[\n\\mathbb{E}(\\text{fertility}) + \\beta_2 = \\beta_0 + \\beta_1 (\\text{education}) + \\beta_2 \\left[(\\text{HDI}) + 1\\right]\n\\]\n\nif education also changes, then the estimated change in fertility is no longer \\(\\beta_2\\)\n\n\n\nSo now the interpretation is: a 0.1 increase in HDI is associated with an estimated decrease in fertility rate of 0.413, after accounting for education."
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#inference",
    "href": "docs/Lectures/week8-mlr.html#inference",
    "title": "Multiple regression",
    "section": "Inference",
    "text": "Inference\nConfidence intervals are the same as before. A 95% interval is:\n\\[\n\\hat{\\beta}_j \\pm 2 SE(\\hat{\\beta}_j)\n\\]\n\n\nmlr.fit().conf_int().rename(columns = {0: 'lwr', 1: 'upr'})\n\n\n\n\n\n\n\n\nlwr\nupr\n\n\n\n\nconst\n7.475988\n8.444753\n\n\neduc_expected_yrs_f\n-0.285437\n-0.118554\n\n\nhdi\n-5.477671\n-2.787574"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#prediction",
    "href": "docs/Lectures/week8-mlr.html#prediction",
    "title": "Multiple regression",
    "section": "Prediction",
    "text": "Prediction\nDitto predictions.\n\n\\(\\hat{y} = \\mathbf{x}'\\hat{\\beta}\\)\n95% interval for the mean: \\(\\widehat{\\mathbb{E}y} \\pm 2 SE\\left(\\widehat{\\mathbb{E}y}\\right)\\)\n95% interval for a predicted observation: \\(\\hat{y} \\pm 2SE(\\hat{y})\\)\n\n\n\nx_new = np.array([1, 10, 0.5])\nmlr.fit().get_prediction(x_new).summary_frame()\n\n\n\n\n\n\n\n\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\n3.874104\n0.119361\n3.63806\n4.110147\n2.685664\n5.062544"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#model-visualizations",
    "href": "docs/Lectures/week8-mlr.html#model-visualizations",
    "title": "Multiple regression",
    "section": "Model visualizations",
    "text": "Model visualizations\nVisualization gets a bit trickier because of the presence of that second variable. Here I plotted the marginal relationship with HDI for three quantiles of education.\n\n\nCode\n# set coordinates for grid mesh\neduc_gridpts = fertility.educ_expected_yrs_f.quantile([0.2, 0.5, 0.8]).values\nhdi_gridpts = np.linspace(fertility.hdi.min(), fertility.hdi.max(), num = 100)\n\n# create prediction grid\ng1, g2 = np.meshgrid(educ_gridpts, hdi_gridpts)\ngrid = np.array([g1.ravel(), g2.ravel()]).T\ngrid_df = pd.DataFrame(grid, columns = ['educ', 'hdi'])\n\n# format for input to get_prediction()\nx_pred = sm.tools.add_constant(grid_df)\n\n# compute predictions\npred_df = mlr.fit().get_prediction(x_pred).summary_frame()\n\n# append to grid\npred_df = pd.concat([grid_df, pred_df], axis = 1)\n\n# plot\nscatter = alt.Chart(fertility).mark_circle(opacity = 0.5).encode(\n    x = alt.X('hdi', \n        scale = alt.Scale(zero = False),\n        title = 'Human development index'),\n    y = alt.Y('fertility_total',\n        title = 'Fertility rate'),\n    color = alt.Color('educ_expected_yrs_f',\n        title = 'Education')\n)\n\nmodel = alt.Chart(pred_df).mark_line().encode(\n    x = 'hdi',\n    y = 'mean',\n    color = 'educ'\n)\n\n(scatter + model).configure_axis(\n    labelFontSize = 14,\n    titleFontSize = 16\n).configure_legend(\n    labelFontSize = 14,\n    titleFontSize = 16\n)"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#urban-tree-cover-data",
    "href": "docs/Lectures/week8-mlr.html#urban-tree-cover-data",
    "title": "Multiple regression",
    "section": "Urban tree cover data",
    "text": "Urban tree cover data\nTree canopy is thought to reduce summer temperatures in urban areas. Consider tree cover and a few other variables measured on a random sample of ~2K census blocks with some canpoy cover in San Diego:\n\n\nCode\ntrees.head(3)\n\n\n\n\n\n\n\n\n\ncensus_block_GEOID\ntree_cover\nmean_summer_temp\nmean_income\npop_density\n\n\n\n\n20504\n6.073020e+13\n25.798277\n33.859524\n66738\nvery low\n\n\n20849\n6.073000e+13\n11.439114\n32.150000\n63189\nvery low\n\n\n10621\n6.073020e+13\n26.661660\n32.404167\n35099\nhigh\n\n\n\n\n\n\n\n\n\nMcDonald RI, Biswas T, Sachar C, Housman I, Boucher TM, Balk D, et al. (2021) The tree cover and temperature disparity in US urbanized areas: Quantifying the association with income across 5,723 communities. PLoS ONE 16(4). doi:10.1371/journal.pone.0249715."
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#simple-regression-analysis",
    "href": "docs/Lectures/week8-mlr.html#simple-regression-analysis",
    "title": "Multiple regression",
    "section": "Simple regression analysis",
    "text": "Simple regression analysis\nIs higher tree cover associated with lower summer temperatures?\n\n\n\n\nCode\nalt.Chart(trees).mark_circle(opacity = 0.3).encode(\n    x = alt.X('tree_cover', \n        title = 'Tree canopy cover (percent)'),\n    y = alt.Y('mean_summer_temp', \n        scale = alt.Scale(zero = False),\n        title = 'Average summer temperature (C)')\n).configure_axis(\n    titleFontSize = 16,\n    labelFontSize = 14\n)\n\n\n\n\n\n\n\n\n\nweak negative association\nlinear approximation seems reasonable\nlots of census blocks with low (&lt;10%) cover"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#simple-regression-analysis-1",
    "href": "docs/Lectures/week8-mlr.html#simple-regression-analysis-1",
    "title": "Multiple regression",
    "section": "Simple regression analysis",
    "text": "Simple regression analysis\nWe can use SLR to quantify the association:\n\n# explanatory and response variables\nx = sm.tools.add_constant(trees.tree_cover)\ny = trees.mean_summer_temp\n\n# fit temp ~ cover\nslr = sm.OLS(endog = y, exog = x)\nslr.fit().params\n\nconst         34.994336\ntree_cover    -0.073114\ndtype: float64\n\n\n\nEach 13.7% increase in tree canopy cover is associated with an estimated 1-degree decrease in mean summer surface temperatures.\n\n\n(How did I get 13.7%?)"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#simple-regression-analysis-2",
    "href": "docs/Lectures/week8-mlr.html#simple-regression-analysis-2",
    "title": "Multiple regression",
    "section": "Simple regression analysis",
    "text": "Simple regression analysis\nConfidence intervals:\n\n\nCode\nslr.fit().conf_int().rename(columns = {0: 'lwr', 1: 'upr'})\n\n\n\n\n\n\n\n\n\nlwr\nupr\n\n\n\n\nconst\n34.876501\n35.112170\n\n\ntree_cover\n-0.083032\n-0.063195\n\n\n\n\n\n\n\n\n\nInterpretation: with 95% confidence, a 10% increase in tree cover is associated with an estimated decrease in mean summer surface temperatures between 0.6 and 0.8 degrees celsius."
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#considering-more-variables",
    "href": "docs/Lectures/week8-mlr.html#considering-more-variables",
    "title": "Multiple regression",
    "section": "Considering more variables",
    "text": "Considering more variables\nIs tree cover associated with affluence and population density?\n\n\n\n\nCode\nalt.Chart(trees).mark_circle(opacity = 0.3).encode(\n    x = alt.X('mean_income', \n        scale = alt.Scale(type = 'log'),\n        title = 'Mean income (USD)'),\n    y = alt.Y('tree_cover', \n        scale = alt.Scale(type = 'log'),\n        title = 'Tree cover (percent)')\n).configure_axis(\n    titleFontSize = 16,\n    labelFontSize = 14\n)\n\n\n\n\n\n\n\n\n\nmaybe with income, not clear about density\nto answer exactly, want to model tree_cover as a function of mean_income and pop_density"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#handling-categorical-variables",
    "href": "docs/Lectures/week8-mlr.html#handling-categorical-variables",
    "title": "Multiple regression",
    "section": "Handling categorical variables",
    "text": "Handling categorical variables\nThe population density is recorded as a categorical variable:\n\n\nCode\nregdata = trees.loc[:, ['tree_cover', 'mean_income', 'pop_density']]\nregdata['log_cover'] = np.log(regdata.tree_cover)\nregdata['log_income'] = np.log(regdata.mean_income)\nregdata.drop(columns = {'tree_cover', 'mean_income'}, inplace = True)\nregdata.head(4)\n\n\n\n\n\n\n\n\n\npop_density\nlog_cover\nlog_income\n\n\n\n\n20504\nvery low\n3.250308\n11.108530\n\n\n20849\nvery low\n2.437039\n11.053886\n\n\n10621\nhigh\n3.283227\n10.465928\n\n\n9031\nlow\n2.311344\n10.309253\n\n\n\n\n\n\n\n\nOn face value, it might seem we could simply fit this model: \\[\n\\underbrace{\\log(\\text{cover}_i)}_{y_i}\n    = \\beta_0 +\n        \\beta_1 \\underbrace{\\log(\\text{income}_i)}_{x_{i1}} +\n        \\beta_2 \\underbrace{\\text{density}_i}_{x_{i2}} + \\epsilon_i\n    \\qquad i = 1, \\dots, 2000\n\\]\n\n\nBut this doesn’t quite make sense, because the values of \\(\\text{density}_i\\) would be words!"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#indicator-variable-encoding",
    "href": "docs/Lectures/week8-mlr.html#indicator-variable-encoding",
    "title": "Multiple regression",
    "section": "Indicator variable encoding",
    "text": "Indicator variable encoding\nThe solution to this issue is to encode each level of the categorical variable separately using a set of indicator variables.\n\nFor instance, to indicate whether a census block is of low population density, we can use the indicator:\n\\[\nI(\\text{density $=$ low}) = \\begin{cases} 1 \\text{ if population density is low} \\\\ 0 \\text{ otherwise}\\end{cases}\n\\]\n\n\nWe can encode the levels of pop_density using a collection of indicators:\n\n\nCode\n# create indicator variables\ndensity_encoded = pd.get_dummies(regdata.pop_density, drop_first = True)\n\n# display example values corresponding to categorical variable\npd.concat([regdata[['pop_density']], density_encoded], axis = 1).groupby('pop_density').head(2)\n\n\n\n\n\n\n\n\n\npop_density\nlow\nmedium\nhigh\n\n\n\n\n20504\nvery low\n0\n0\n0\n\n\n20849\nvery low\n0\n0\n0\n\n\n10621\nhigh\n0\n0\n1\n\n\n9031\nlow\n1\n0\n0\n\n\n1004\nhigh\n0\n0\n1\n\n\n15939\nlow\n1\n0\n0\n\n\n6647\nmedium\n0\n1\n0\n\n\n17963\nmedium\n0\n1\n0"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#the-mlr-model-with-indicators",
    "href": "docs/Lectures/week8-mlr.html#the-mlr-model-with-indicators",
    "title": "Multiple regression",
    "section": "The MLR model with indicators",
    "text": "The MLR model with indicators\nThe model with the encoded population density variable is:\n\\[\n\\underbrace{\\log(\\text{cover}_i)}_{y_i}\n    = \\beta_0 +\n        \\beta_1 \\underbrace{\\log(\\text{income}_i)}_{x_{i1}} +\n        \\beta_2 \\underbrace{\\text{low}_i}_{x_{i2}} +\n        \\beta_3 \\underbrace{\\text{med}_i}_{x_{i3}} +\n        \\beta_4 \\underbrace{\\text{high}_i}_{x_{i4}} +\n        \\epsilon_i\n\\]\n\nThe effect is that the model has different intercepts for each population density.\n\\[\n\\begin{align*}\n\\text{density $=$ very low}\n    &\\quad\\Longrightarrow\\quad\n    \\mathbb{E}\\log(\\text{cover}_i)\n        = \\underbrace{\\beta_0}_\\text{intercept} + \\beta_1\\log(\\text{income}_i) \\\\\n\\text{density $=$ low}\n    &\\quad\\Longrightarrow\\quad\n    \\mathbb{E}\\log(\\text{cover}_i)\n        = \\underbrace{\\beta_0 + \\beta_2}_\\text{intercept} + \\beta_1\\log(\\text{income}_i) \\\\\n\\text{density $=$ medium}\n    &\\quad\\Longrightarrow\\quad\n    \\mathbb{E}\\log(\\text{cover}_i)\n        = \\underbrace{\\beta_0 + \\beta_3}_\\text{intercept} + \\beta_1\\log(\\text{income}_i) \\\\\n\\text{density $=$ high}\n    &\\quad\\Longrightarrow\\quad\n    \\mathbb{E}\\log(\\text{cover}_i)\n        = \\underbrace{\\beta_0 + \\beta_4}_\\text{intercept} + \\beta_1\\log(\\text{income}_i)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#in-matrix-form",
    "href": "docs/Lectures/week8-mlr.html#in-matrix-form",
    "title": "Multiple regression",
    "section": "In matrix form",
    "text": "In matrix form\nThe explanatory variable matrix \\(\\mathbf{X}\\) for this model will be of the form:\n\\[\n\\mathbf{X} = \\left[\\begin{array}{c:cccc}\n    1 &\\log(\\text{income}_{1}) &\\text{low}_1 &\\text{med}_1 &\\text{high}_1 \\\\\n    1 &\\log(\\text{income}_{2}) &\\text{low}_2 &\\text{med}_2 &\\text{high}_2 \\\\\n    \\vdots &\\vdots &\\vdots &\\vdots &\\vdots \\\\\n    1 &\\log(\\text{income}_{2000}) &\\text{low}_{2000} &\\text{med}_{2000} &\\text{high}_{2000}\n    \\end{array}\\right]\n\\]\n\n\n\nCode\n# form explanatory variable matrix\nx_vars = pd.concat([regdata.log_income, density_encoded], axis = 1)\nx = sm.tools.add_constant(x_vars)\nx.head(4)\n\n\n\n\n\n\n\n\n\nconst\nlog_income\nlow\nmedium\nhigh\n\n\n\n\n20504\n1.0\n11.108530\n0\n0\n0\n\n\n20849\n1.0\n11.053886\n0\n0\n0\n\n\n10621\n1.0\n10.465928\n0\n0\n1\n\n\n9031\n1.0\n10.309253\n1\n0\n0"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#fit-summary",
    "href": "docs/Lectures/week8-mlr.html#fit-summary",
    "title": "Multiple regression",
    "section": "Fit summary",
    "text": "Fit summary\nThe remaining calculations are all the same as before. Here is the model fit summary:\n\n\nCode\n# form explanatory variable matrix\nx_vars = pd.concat([regdata.log_income, density_encoded], axis = 1)\nx = sm.tools.add_constant(x_vars)\ny = regdata.log_cover\n\n# fit model\nmlr = sm.OLS(endog = y, exog = x)\nrslt = mlr.fit()\n\n# summarize fit\ncoef_tbl = pd.DataFrame({\n    'estimate': rslt.params,\n    'standard error': np.sqrt(rslt.cov_params().values.diagonal())\n}, index = x.columns.values)\ncoef_tbl.loc['error variance', 'estimate'] = rslt.scale\n\ncoef_tbl\n\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\nconst\n-4.751870\n0.621673\n\n\nlog_income\n0.631469\n0.058290\n\n\nlow\n-0.414486\n0.070483\n\n\nmedium\n-0.675626\n0.079552\n\n\nhigh\n-0.606980\n0.101708\n\n\nerror variance\n1.552578\nNaN\n\n\n\n\n\n\n\n\nincome is positively associated with tree cover\nbut how do you interpret the coefficients for the indicator variables?"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#interpreting-indicator-coefficients",
    "href": "docs/Lectures/week8-mlr.html#interpreting-indicator-coefficients",
    "title": "Multiple regression",
    "section": "Interpreting indicator coefficients",
    "text": "Interpreting indicator coefficients\nEach level of the categorical variable has its own intercept:\n\\[\n\\begin{align*}\n\\text{very low density:}\\quad &\\mathbb{E}\\log(\\text{cover}) = \\beta_0 + \\beta_1 \\log(\\text{income}) \\\\\n\\text{low density:}\\quad &\\mathbb{E}\\log(\\text{cover}) = (\\beta_0 + \\beta_2) + \\beta_1 \\log(\\text{income}) \\\\\n\\text{medium density:}\\quad &\\mathbb{E}\\log(\\text{cover}) = (\\beta_0 + \\beta_3) + \\beta_1 \\log(\\text{income}) \\\\\n\\text{high density:}\\quad &\\mathbb{E}\\log(\\text{cover}) = (\\beta_0 + \\beta_4) + \\beta_1 \\log(\\text{income})\n\\end{align*}\n\\]\n\n\\(\\beta_0\\) is the intercept when density is very low – this gets called the reference level\n\\(\\beta_2, \\beta_3, \\beta_4\\) are interpreted relative to the reference level:\n\n\\(\\beta_2\\) is the difference in expected log cover between low density and very low density blocks, after accounting for income\n\\(\\beta_3\\) is the difference in expected log cover between medium density and very low density blocks, after accounting for income\n\\(\\beta_4\\) is the difference in expected log cover between high density and very low density blocks, after accounting for income"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#interpreting-estimates",
    "href": "docs/Lectures/week8-mlr.html#interpreting-estimates",
    "title": "Multiple regression",
    "section": "Interpreting estimates",
    "text": "Interpreting estimates\n\n\n\n\n\n\n\n\n\nestimate\nstandard error\n\n\n\n\nconst\n-4.751870\n0.621673\n\n\nlog_income\n0.631469\n0.058290\n\n\nlow\n-0.414486\n0.070483\n\n\nmedium\n-0.675626\n0.079552\n\n\nhigh\n-0.606980\n0.101708\n\n\nerror variance\n1.552578\nNaN\n\n\n\n\n\n\n\n\neach doubling of mean income is associated with an estimated 55% increase in median tree cover, after accounting for population density\ncensus blocks with higher population densities are estimated as having a median tree canopy up to 50% lower than census blocks with very low population densities, after accounting for mean income"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#prediction-1",
    "href": "docs/Lectures/week8-mlr.html#prediction-1",
    "title": "Multiple regression",
    "section": "Prediction",
    "text": "Prediction\nPrediction works the same way, but we need to supply the indicator encoding rather than the categorical variable level.\n\nx_new = np.array([1, np.log(115000), 0, 1, 0])\npred = rslt.get_prediction(x_new)\nnp.exp(pred.summary_frame())\n\n\n\n\n\n\n\n\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\nobs_ci_lower\nobs_ci_upper\n\n\n\n\n0\n6.895105\n1.102304\n5.696153\n8.346419\n0.594349\n79.990893\n\n\n\n\n\n\n\nCheck your understanding and fill in the blanks:\n\nthe median tree cover for a _______ density census block with mean income _______ is estimated to be between _______ and _______ percent\nthe tree cover for a _______ density census block with mean income _______ is estimated to be between _______ and _______ percent"
  },
  {
    "objectID": "docs/Lectures/week8-mlr.html#comments-on-scope-of-inference",
    "href": "docs/Lectures/week8-mlr.html#comments-on-scope-of-inference",
    "title": "Multiple regression",
    "section": "Comments on scope of inference",
    "text": "Comments on scope of inference\nThe data in this case study are from a random sample of census blocks in the San Diego urban area.\n\nThey are therefore representative of all census blocks in the San Diego urban area.\n\n\n\nThe model approximates the actual associations between summer temperatures, tree cover, income, and population density in the region."
  }
]